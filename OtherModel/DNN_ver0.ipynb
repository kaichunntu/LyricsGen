{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as ly\n",
    "from tensorflow.contrib.framework import nest \n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "sess_opt = tf.ConfigProto(gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95 , allow_growth=True)\n",
    "                         ,device_count={'GPU': 1})\n",
    "\n",
    "from utils import exist_or_mkdir , data_manager , transform_orig\n",
    "\n",
    "exp_folder = \"DNN_ver0\"\n",
    "model_path = \"model_para\"\n",
    "tmp_path = \"tmp\"\n",
    "log_path = \"log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path : './DNN_ver0'\n",
      "Path : './DNN_ver0/model_para'\n",
      "Path : './DNN_ver0/tmp'\n",
      "Path : './DNN_ver0/log'\n"
     ]
    }
   ],
   "source": [
    "exp_folder = exist_or_mkdir(\"./\",exp_folder)\n",
    "model_path = exist_or_mkdir(exp_folder,model_path)\n",
    "tmp_path = exist_or_mkdir(exp_folder,tmp_path)\n",
    "log_path = exist_or_mkdir(exp_folder,log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder_max_len = 50\n",
    "Decoder_max_len = 25\n",
    "min_count = 3\n",
    "print_interval = 1500\n",
    "train_batch_size = 128\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################################################################\n",
      "########################################  Loading Data  ##########################################\n",
      "##################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "##################################################################################################\n",
    "########################################  Loading Data  ##########################################\n",
    "##################################################################################################\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading Train Data ###\n",
      "Data count : 651339\n",
      "\n",
      "### Data view ###\n",
      "Original data  : ['SOS', '人潮', '中', '怕', '失散', '所以', '轻轻', '拉', '你', '的', '手', 'EOS', 'm', 'd', 'v', 'd', 'v', 'NOP', 'ong', 'NOE', '5', 'NOR']\n",
      "Output Sentence : ['SOS', '一刻', '不', '放松', '不', '放松', 'EOS']\n",
      "### Loading Test Data ###\n",
      "Data count : 70000\n",
      "\n",
      "### Data view ###\n",
      "Original data  : ['SOS', '你', '手中', '的', '温暖', '我', '好', '想', '触摸', 'EOS', 'i', 'f', 'r', 'p', 'r', 'v', 'NOP', 'eng', 'NOE', '6', 'NOR']\n"
     ]
    }
   ],
   "source": [
    "train_path = [\"data/{}/train.csv\".format(x) for x in [\"all\"]]\n",
    "test_path = [\"data/{}/test.csv\".format(x) for x in [\"all\"]]\n",
    "\n",
    "print(\"### Loading Train Data ###\")\n",
    "data_agent = data_manager(train_path , train=True)\n",
    "\n",
    "\n",
    "print(\"### Loading Test Data ###\")\n",
    "test_agent = data_manager(test_path , train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Preprocessing ###\n",
      "Min Count : 3\n",
      "Max Length : [50, 25]\n",
      "Word Count : 59465\n",
      "Orig data  : ['SOS', '只有', '这', '秦腔', 'EOS', 'd', 'v', 'v', 'r', 'n', 'NOP', 'iang', 'NOE', '5', 'NOR']\n",
      "Index data : [380, 95, 10514, 3, 17, 15, 15, 12, 31, 18, 190, 20, 79, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Output Orig data  : ['SOS', '也', '只能', '有', '这', '秦腔', 'EOS']\n",
      "Output Index data : [68, 1372, 318, 95, 10514, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Preprocessing ###\")\n",
    "idx_in_sen , idx_out_sen , mask_in , mask_out , length_in , length_out , idx2word , word2idx , remain_idx =\\\n",
    "    transform_orig([data_agent.orig_data,data_agent.out_sen],min_count=min_count, max_len = [Encoder_max_len,Decoder_max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({\"orig_word\":[idx2word,word2idx] },\n",
    "            open(os.path.join(tmp_path,\"tokenizer.pkl\") , \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################################################################\n",
      "#######################################  Building Model  #########################################\n",
      "##################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "##################################################################################################\n",
    "#######################################  Building Model  #########################################\n",
    "##################################################################################################\n",
    "\"\"\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_encode(_inputs,dim,length,b,reuse_flag=False,name=None):\n",
    "    inputs = tf.reshape(_inputs,[-1,int(_inputs.shape[-1])])\n",
    "    fc1 = tf.layers.dense(inputs , dim , activation=tf.nn.leaky_relu,name=\"{}_fc1\".format(name),reuse=reuse_flag)\n",
    "    fc2 = tf.layers.dense(fc1 , dim , activation=None,name=\"{}_fc2\".format(name),reuse=reuse_flag)\n",
    "    f = tf.layers.dense(fc2,dim,name=\"{}_fc3\".format(name),reuse=reuse_flag)\n",
    "    \n",
    "    c = tf.matmul(f,fc2,transpose_b=True)\n",
    "    mask = tf.concat([tf.ones([length,length]) , tf.zeros([length,(b-1)*length])],axis=-1)\n",
    "    time = tf.constant(1,tf.int32)\n",
    "    def body(time,mask):\n",
    "        tmp = tf.concat([tf.zeros([length,length*time]),\n",
    "                         tf.ones([length,length]),\n",
    "                         tf.zeros([length,(b-time-1)*length])],\n",
    "                        axis=-1)\n",
    "        return time+1 , tf.concat([mask,tmp],axis=0)\n",
    "    _ , mask = tf.while_loop(lambda t,*_:t<b,body,loop_vars=[time,mask])\n",
    "    \n",
    "    exp_c = tf.exp(c)*mask\n",
    "    c = exp_c/(tf.reduce_sum(exp_c,axis=-1,keepdims=True)+1e-10)\n",
    "    ws = c@fc2\n",
    "    return tf.reshape(ws+fc1,[-1,length,dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(inputs,encoder_vec,max_len,residual=None,dim=256,kernel_size=3,reuse_flag=True):\n",
    "    print(\"Deconv inputs : \" , inputs)\n",
    "    with tf.name_scope(\"Deconv\"):\n",
    "        BA,MA,DIM = tf.unstack(tf.shape(inputs))\n",
    "        vec_o = attn_encode(inputs,dim,MA,BA,reuse_flag=True,name=\"EN_layer0\")\n",
    "        f_vec = tf.layers.dense(tf.reshape(vec_o[:,-1,:],[BA,1,dim]),dim,activation=tf.nn.leaky_relu)\n",
    "        f_vec = tf.concat([vec_o,tf.tile(f_vec,[1,MA,1])],axis=-1)\n",
    "        z = tf.layers.dense(f_vec,128,activation=tf.nn.leaky_relu)\n",
    "        z = tf.layers.dense(z,1,activation=None)\n",
    "        z = tf.nn.softmax(z,axis=1)\n",
    "        vec_o = tf.reduce_sum(vec_o*z,axis=1,keepdims=True)\n",
    "        \n",
    "#         vec_o = tf.layers.dense(inputs,int(encoder_vec.shape[-1]),activation=tf.nn.leaky_relu)\n",
    "        print(vec_o)\n",
    "        vec = tf.concat([tf.tile(vec_o,[1,max_len,1]),encoder_vec],axis=-1)\n",
    "        attn_score = tf.layers.dense(vec,dim//2,activation=tf.nn.leaky_relu)\n",
    "        attn_score = tf.nn.softmax(tf.layers.dense(attn_score,1,activation=None),axis=1)\n",
    "        out = tf.reduce_sum(encoder_vec*attn_score,axis=1,keepdims=True)\n",
    "    print(\"Deconv out :\" , out)\n",
    "    print(\"Deconv score :\" , attn_score)\n",
    "    return out , attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordclf(inputs,dim,embd_T,reuse_flag=False,training=True):\n",
    "    with tf.name_scope(\"WordClf\"):\n",
    "        print(\"Wordclf inputs : \" ,inputs)\n",
    "        fc1 = tf.layers.dense(inputs,dim,activation=None,name=\"fc1\",reuse=reuse_flag)\n",
    "    #     fc1 = tf.layers.batch_normalization(fc1,trainable=training,training=training,\n",
    "    #                                         reuse=reuse_flag,name=\"BN1\")\n",
    "        fc1 = tf.nn.leaky_relu(fc1)\n",
    "        fc2 = tf.layers.dense(fc1,embd_dim,activation=None,reuse=reuse_flag,name=\"fc2\")\n",
    "        print(fc2)\n",
    "        flat = tf.layers.flatten(fc2)\n",
    "        print(flat)\n",
    "        out = flat @ embd_T\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_deconv(encoder,embd,embd_T,max_time):\n",
    "    \n",
    "    batch_size,max_len,encoder_dim = tf.unstack(tf.shape(encoder))\n",
    "    \n",
    "    emit_ta = nest.pack_sequence_as(int(embd.shape[0]),\n",
    "                                    [tensor_array_ops.TensorArray(tf.float32,\n",
    "                                                                  clear_after_read=False,\n",
    "                                                                  size=0,\n",
    "                                                                  dynamic_size=True,\n",
    "                                                                  element_shape=tensor_shape.\\\n",
    "                                                                      TensorShape([None,len(idx2word)]))])\n",
    "    emit_input = nest.pack_sequence_as(int(1),\n",
    "                                           [tensor_array_ops.TensorArray(tf.int32,\n",
    "                                                                         clear_after_read=False,\n",
    "                                                                         size=0,\n",
    "                                                                         dynamic_size=True,\n",
    "                                                                         element_shape=tensor_shape.\\\n",
    "                                                                             TensorShape([None]))])\n",
    "    \n",
    "    emit_score = tensor_array_ops.TensorArray(tf.float32,\n",
    "                                              clear_after_read=False,\n",
    "                                              size=0,\n",
    "                                              dynamic_size=True,\n",
    "                                              element_shape=tensor_shape.\\\n",
    "                                                  TensorShape([None,None,1]))\n",
    "    \n",
    "    time = tf.constant(0,dtype=tf.int32)\n",
    "    output_time = tf.constant(0,dtype=tf.int32)\n",
    "    def initialize(batch_size,time,emit_input):\n",
    "        for w in [\"SOS\"]:\n",
    "            idx = tf.reshape(tf.constant(word2idx[w],dtype=tf.int32),[-1])\n",
    "            idx = tf.tile(idx,[batch_size])\n",
    "            emit_input = nest.map_structure(lambda ta,em: ta.write(time,em),emit_input,idx)\n",
    "            time += 1\n",
    "        return emit_input,time\n",
    "    \n",
    "    emit_input, time = initialize(batch_size,time,emit_input)\n",
    "    \n",
    "    def body(output_time,time,emit_input,emit_ta,emit_score):\n",
    "        \n",
    "#         inputs_idx = tf.transpose(emit_input.gather([time-1]),[1,0])\n",
    "        inputs_idx = tf.transpose(emit_input.stack(),[1,0])\n",
    "        print(\"input idx\",inputs_idx)\n",
    "        \n",
    "        inputs_vec = tf.nn.embedding_lookup(embd,inputs_idx)\n",
    "        output_vec , attn_s= deconv(inputs_vec,encoder,max_len,dim=256,reuse_flag=False)\n",
    "        output_logits = wordclf(output_vec,300,embd_T,reuse_flag=False)\n",
    "        \n",
    "        next_idx = tf.argmax(output_logits,axis=-1,output_type=tf.int32)\n",
    "        emit_input = nest.map_structure(lambda ta,em:ta.write(time,em),emit_input,next_idx)\n",
    "        time += 1\n",
    "        \n",
    "        emit_ta = emit_ta.write(output_time,output_logits)\n",
    "        emit_score = emit_score.write(output_time,attn_s)\n",
    "        output_time += 1\n",
    "        \n",
    "        return output_time,time,emit_input,emit_ta,emit_score\n",
    "    \n",
    "    def condition(t,*_):\n",
    "        return t<max_time\n",
    "    \n",
    "    _,_,emit_input,emit_ta,emit_score = \\\n",
    "            tf.while_loop(condition,body,\n",
    "                          loop_vars=[output_time,time,emit_input,emit_ta,emit_score],\n",
    "                          swap_memory=False)\n",
    "        \n",
    "    emit_input = tf.transpose(emit_input.stack(),[1,0])[:,1::]\n",
    "    emit_ta = tf.transpose(emit_ta.stack(),[1,0,2])\n",
    "    emit_score = tf.transpose(emit_score.stack(),[1,0,2,3])\n",
    "    emit_score = tf.reshape(emit_score , [batch_size,-1,max_len])\n",
    "    return emit_input,emit_ta,emit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm(_t_position,_p_position,pos_tensor):\n",
    "    grad = tf.gradients(logits[0,_t_position,_p_position],pos_tensor)[0]\n",
    "    return tf.sqrt(tf.reduce_sum(grad*grad,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input idx Tensor(\"DNN_decode/while/transpose:0\", shape=(?, ?), dtype=int32)\n",
      "Deconv inputs :  Tensor(\"DNN_decode/while/embedding_lookup:0\", shape=(?, ?, 200), dtype=float32)\n",
      "Tensor(\"DNN_decode/while/Deconv/Sum_1:0\", shape=(?, 1, 256), dtype=float32)\n",
      "Deconv out : Tensor(\"DNN_decode/while/Deconv/Sum_2:0\", shape=(?, 1, 256), dtype=float32)\n",
      "Deconv score : Tensor(\"DNN_decode/while/Deconv/transpose_3:0\", shape=(?, ?, 1), dtype=float32)\n",
      "Wordclf inputs :  Tensor(\"DNN_decode/while/Deconv/Sum_2:0\", shape=(?, 1, 256), dtype=float32)\n",
      "Tensor(\"DNN_decode/while/WordClf/fc2/BiasAdd:0\", shape=(?, 1, 200), dtype=float32)\n",
      "Tensor(\"DNN_decode/while/WordClf/flatten/Reshape:0\", shape=(?, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embd_dim = 200\n",
    "L0_dim = 256\n",
    "L1_dim = 256\n",
    "L2_dim = 300\n",
    "clf_dim = 256\n",
    "\n",
    "g_conv = tf.Graph()\n",
    "with g_conv.as_default() as g:\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        _xs = tf.placeholder(tf.int32,[None,None])\n",
    "        _xs_length = tf.placeholder(tf.int32,[None])\n",
    "        xs_mask = tf.sequence_mask(_xs_length,dtype=tf.float32)\n",
    "        \n",
    "        _ys = tf.placeholder(tf.int32,[None,None])\n",
    "        ys_one_hot = tf.one_hot(_ys,depth=len(idx2word))\n",
    "        _ys_length = tf.placeholder(tf.int32,[None])\n",
    "        ys_mask = tf.sequence_mask(_ys_length,dtype=tf.float32)\n",
    "        \n",
    "        t_place = tf.placeholder(tf.int32,[])\n",
    "        _exp_idx = tf.placeholder(tf.int32,[None])\n",
    "        \n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"Embedding\"):\n",
    "        _embd = tf.get_variable(\"Embedding\",[len(idx2word),embd_dim],dtype=tf.float32,trainable=True,\n",
    "                                initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        _embd_T = tf.transpose(_embd,[1,0])\n",
    "        x_vector = tf.nn.embedding_lookup(_embd, _xs, max_norm=5)\n",
    "        BatchSize , MAX_LEN , DIM = tf.unstack(tf.shape(x_vector))\n",
    "        _exp_vec = tf.nn.embedding_lookup(_embd,_exp_idx)\n",
    "        \n",
    "    with tf.name_scope(\"DNN_encode\"):\n",
    "        attn_dnn0 = attn_encode(x_vector,L0_dim,MAX_LEN,BatchSize,name=\"EN_layer0\")\n",
    "#         attn_dnn1 = attn_encode(attn_dnn0,L1_dim,MAX_LEN,BatchSize,name=\"EN_layer1\")\n",
    "        attn_dnn1 = attn_dnn0\n",
    "        \n",
    "    with tf.name_scope(\"DNN_decode\"):\n",
    "        _pred , logits , _attn_map = dynamic_deconv(attn_dnn1,_embd,_embd_T,t_place)\n",
    "    \n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ys_one_hot,logits=logits)*ys_mask\n",
    "        ce = tf.reduce_sum(ce,axis=1,keepdims=False)/tf.cast(_ys_length,tf.float32)\n",
    "        _loss = tf.reduce_mean(ce)\n",
    "        \n",
    "    with tf.name_scope(\"Train\"):\n",
    "        g_step = tf.Variable(0,dtype=tf.int32,trainable=False,name=\"Global_step\")\n",
    "        lr = tf.train.exponential_decay(2e-4,g_step,2000,0.95,staircase=True)\n",
    "        opt = tf.train.AdamOptimizer(lr)\n",
    "        allgrads = opt.compute_gradients(_loss) \n",
    "        clip_grads = [ ( tf.clip_by_norm(grad,2) ,var) for grad , var in allgrads]\n",
    "        _update = opt.apply_gradients(clip_grads,global_step=g_step)\n",
    "#         _update = opt.minimize(_loss,global_step=g_step)\n",
    "        _global_step_assign = tf.placeholder(tf.int32)\n",
    "        assign_g_step = g_step.assign_add(_global_step_assign)\n",
    "    with tf.name_scope(\"Gradient\"):\n",
    "        _t_position = tf.placeholder(tf.int32)\n",
    "        _p_position = _pred[0,_t_position]\n",
    "        xs_vector_gradnorm = get_grad_norm(_t_position,_p_position,x_vector)\n",
    "        dnn1_gradnorm = get_grad_norm(_t_position,_p_position,attn_dnn0)\n",
    "        dnn2_gradnorm = get_grad_norm(_t_position,_p_position,attn_dnn1)\n",
    "    \n",
    "    all_var = tf.trainable_variables()\n",
    "    _init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=8,var_list=tf.trainable_variables())\n",
    "#     tf.summary.FileWriter(log_path,graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consume time : 2.35\n",
      "\n",
      "Var Count : 12410718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12410718"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Consume time : {:.2f}\".format(time.time()-start_time))\n",
    "print()\n",
    "def calculate_var(var):\n",
    "    d = 0\n",
    "    for w in var:\n",
    "        tmp_d = 1\n",
    "        for ww in w.shape:\n",
    "            tmp_d *= int(ww)\n",
    "        d += tmp_d\n",
    "    print(\"Var Count :\" , d)\n",
    "    return d\n",
    "\n",
    "calculate_var(all_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################################################################\n",
      "#######################################  Start Training  #########################################\n",
      "##################################################################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "##################################################################################################\n",
    "#######################################  Start Training  #########################################\n",
    "##################################################################################################\\n\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=g_conv,config=sess_opt)\n",
    "sess.run(_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN_ver0/model_para/model_22.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,os.path.join(model_path,\"model_22.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111870"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define global step\n",
    "sess.run(assign_g_step,feed_dict={_global_step_assign:22*5085})\n",
    "sess.run(g_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(i):\n",
    "    tmp_end = max(length_in[i])\n",
    "    out_max_len = max(length_out[i])\n",
    "    my_dict = {\n",
    "        _xs:idx_in_sen[i,:tmp_end],\n",
    "        _ys:idx_out_sen[i,:out_max_len],\n",
    "        _ys_length:length_out[i],\n",
    "        t_place:out_max_len\n",
    "    }\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "def evaluate_batch(sess,_pred,count=3):\n",
    "    idx = np.random.choice(idx_in_sen.shape[0],[count])\n",
    "    tmp_max_len = max(length_in[idx])\n",
    "    my_dict = {\n",
    "        _xs:idx_in_sen[idx,:tmp_max_len],\n",
    "        t_place:30\n",
    "    }\n",
    "    pred = sess.run(_pred , feed_dict=my_dict)\n",
    "    \n",
    "    word_seq = []\n",
    "    for i in range(3):\n",
    "        idx_sen = pred[i]\n",
    "        tmp = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            if(idx_sen[t] == 3):\n",
    "                break\n",
    "            tmp.append(idx2word[idx_sen[t]])\n",
    "        word_seq.append(tmp)\n",
    "    \n",
    "    print(\"Max length :\" , tmp_max_len)\n",
    "    for i in range(3):\n",
    "        print(\"  Input word  :\" , \" \".join(data_agent.orig_data[remain_idx[idx[i]]]))\n",
    "        print(\"  Input index :\" , idx_in_sen[idx[i],:tmp_max_len])\n",
    "        print(\"  Ground word :\" , \" \".join(data_agent.out_sen[remain_idx[idx[i]]]))\n",
    "        print(\"    Output    :\" , \" \".join(word_seq[i]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length : 20\n",
      "  Input word  : SOS 就算 寂寞 分手 也 不要 做 朋友 EOS v n d v a r n NOP uo NOE 7 NOR\n",
      "  Input index : [1018  753 2542   68  532  885  861    3   15   31   17   15   40   12\n",
      "   31   18  461   20   66   22]\n",
      "  Ground word : SOS 就算 宇宙 早就 安排 好 这 结果 EOS\n",
      "    Output    : 我会 我 我 我 我 太 不错\n",
      "\n",
      "  Input word  : SOS 我们 任由 回忆 秒杀 将 泪痕 记下 EOS p v uj v n NOP ua NOE 5 NOR\n",
      "  Input index : [  200  5255   118 46861   297  8162 18879     3    13    15    41    15\n",
      "    31    18  1268    20    79    22     0     0]\n",
      "  Ground word : SOS 把 分手 的 说法 理论化 EOS\n",
      "    Output    : 在 在 的 的 笑话\n",
      "\n",
      "  Input word  : SOS 阿嬷 的 泪水 开始 流 EOS d v NOP ao NOE 2 NOR\n",
      "  Input index : [42470    10  1374  1086  1362     3    17    15    18  1224    20   126\n",
      "    22     0     0     0     0     0     0     0]\n",
      "  Ground word : SOS 轻声 说道 EOS\n",
      "    Output    : 从来不 说道\n",
      "\n",
      "Max length : 24\n",
      "  Input word  : SOS 王心凌 EOS v uj n NOP a NOE 3 NOR\n",
      "  Input index : [34194     3    15    41    31    18    40    20    83    22     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "  Ground word : SOS 花 的 嫁纱 EOS\n",
      "    Output    : 有 的 OOV\n",
      "\n",
      "  Input word  : SOS 曾 被 破碎 过 的 心 让 你 今天 轻轻 贴近 EOS m v c v d uj v NOP eng NOE 7 NOR\n",
      "  Input index : [ 2086    46  2262   245    10   608    84     5  1365   717 15229     3\n",
      "    14    15    64    15    17    41    15    18   375    20    66    22]\n",
      "  Ground word : SOS 多少 安慰 及 疑问 偷偷 的 再生 EOS\n",
      "    Output    : 多少 的 的 的 的 的 可能\n",
      "\n",
      "  Input word  : SOS 跟着 风吹 得 方向 去 旅行 EOS d v r r NOP i NOE 4 NOR\n",
      "  Input index : [2218 1459  516  845  202   99    3   17   15   12   12   18   32   20\n",
      "   73   22    0    0    0    0    0    0    0    0]\n",
      "  Ground word : SOS 就 来到 我 这里 EOS\n",
      "    Output    : 我 都 我 你\n",
      "\n",
      "Max length : 16\n",
      "  Input word  : SOS 强悍 到 吓 走 热情 EOS v n d NOP an NOE 3 NOR\n",
      "  Input index : [6744  387 7645  509  727    3   15   31   17   18  136   20   83   22\n",
      "    0    0]\n",
      "  Ground word : SOS 做 女人 极难 EOS\n",
      "    Output    : 不 不 OOV\n",
      "\n",
      "  Input word  : SOS 渐渐 用 所有 美丽 的 时间 EOS n m uj a NOP ei NOE 4 NOR\n",
      "  Input index : [1794 1015  551 2152   10  488    3   31   14   41   40   18  186   20\n",
      "   73   22]\n",
      "  Ground word : SOS 定格 百分百 的 完美 EOS\n",
      "    Output    : 一个 的 的 滋味\n",
      "\n",
      "  Input word  : SOS 重回 故里 千百遍 EOS v n y v a NOP uan NOE 5 NOR\n",
      "  Input index : [ 7654 13366 10462     3    15    31   563    15    40    18   194    20\n",
      "    79    22     0     0]\n",
      "  Ground word : SOS 往事 随风 兮 流 远 EOS\n",
      "    Output    : 往事 随风 兮 呀 转\n",
      "\n",
      "Max length : 19\n",
      "  Input word  : SOS 是 谁 趁着 情浓 圈 我 入帐 中 EOS v m v m n NOP ong NOE 5 NOR\n",
      "  Input index : [   96   254  2736  8988  4650    36 23272   680     3    15    14    15\n",
      "    14    31    18   439    20    79    22]\n",
      "  Ground word : SOS 织 一场 梦造 一座 笼 EOS\n",
      "    Output    : 织 一座 OOV 一场 笼\n",
      "\n",
      "  Input word  : SOS 唱 不尽 的 明天 EOS n a p r nr NOP uan NOE 5 NOR\n",
      "  Input index : [1416 3306   10   98    3   31   40   13   12   39   18  194   20   79\n",
      "   22    0    0    0    0]\n",
      "  Ground word : SOS 母亲 殷切 把 我们 呼唤 EOS\n",
      "    Output    : 我 在 为 在 远\n",
      "\n",
      "  Input word  : SOS 簌簌 的 簌簌 的 EOS d r v NOP an NOE 3 NOR\n",
      "  Input index : [37947    10 37947    10     3    17    12    15    18   136    20    83\n",
      "    22     0     0     0     0     0     0]\n",
      "  Ground word : SOS 将 它们 吹散 EOS\n",
      "    Output    : 我 是 吹散\n",
      "\n",
      "Max length : 21\n",
      "  Input word  : SOS 你 说 你 爱 了 不该 爱 的 人 EOS r uj s a v n NOP en NOE 6 NOR\n",
      "  Input index : [   5  134    5   54   52 4669   54   10  454    3   12   41   72   40\n",
      "   15   31   18  694   20   33   22]\n",
      "  Ground word : SOS 你 的 心中 满 是 伤痕 EOS\n",
      "    Output    : 你 的 心里 心里 的 伤痕\n",
      "\n",
      "  Input word  : SOS 痛 了 自己 EOS r d v v d v d d v NOP u NOE 9 NOR\n",
      "  Input index : [2401   52  264    3   12   17   15   15   17   15   17   17   15   18\n",
      "   19   20   21   22    0    0    0]\n",
      "  Ground word : SOS 我 不 愿 放弃 却 要 故意 默默 允许 EOS\n",
      "    Output    : 我 不 不 不 不 不 不 不 去\n",
      "\n",
      "  Input word  : SOS 啊 一片 春心 付 海棠 EOS m v v v NOP ui NOE 4 NOR\n",
      "  Input index : [  977  1023 11562 14396  4002     3    14    15    15    15    18   279\n",
      "    20    73    22     0     0     0     0     0     0]\n",
      "  Ground word : SOS 半 醉 半醒 应不悔 EOS\n",
      "    Output    : 一 一 一 应不悔\n",
      "\n",
      "Max length : 20\n",
      "  Input word  : SOS 也许 当时 忙 着 微笑 和 哭泣 EOS v uz v n f uj n NOP ing NOE 7 NOR\n",
      "  Input index : [ 396 2975  493   44  231  385  559    3   15   38   15   31   16   41\n",
      "   31   18   65   20   66   22]\n",
      "  Ground word : SOS 忙 着 追逐 天空 中 的 流星 EOS\n",
      "    Output    : 忙 着 着 着 中 的 眼睛\n",
      "\n",
      "  Input word  : SOS 我 轻轻 的 说 EOS r v r v uj n NOP i NOE 6 NOR\n",
      "  Input index : [ 36 717  10 134   3  12  15  12  15  41  31  18  32  20  33  22   0   0\n",
      "   0   0]\n",
      "  Ground word : SOS 你 是 我 要 的 诗 EOS\n",
      "    Output    : 我 的 你 你 的 你\n",
      "\n",
      "  Input word  : SOS 让 旋律 为 这 世界 EOS v f vn uj n NOP ian NOE 5 NOR\n",
      "  Input index : [  84 4354  152   95  483    3   15   16   93   41   31   18  124   20\n",
      "   79   22    0    0    0    0]\n",
      "  Ground word : SOS 画 上 微笑 的 曲线 EOS\n",
      "    Output    : 生命 的 生命 的 画面\n",
      "\n",
      "Max length : 18\n",
      "  Input word  : SOS 共行 才 觅到 自己 EOS c r r m d v v r NOP i NOE 8 NOR\n",
      "  Input index : [1112  652 6007  264    3   64   12   12   14   17   15   15   12   18\n",
      "   32   20   42   22]\n",
      "  Ground word : SOS 若 我 每 一分钟 都 愿能 见 你 EOS\n",
      "    Output    : 我 你 一生 一生 你 你 你 你\n",
      "\n",
      "  Input word  : SOS 我 曾 说 的 天荒地老 EOS n v r v NOP ao NOE 4 NOR\n",
      "  Input index : [  36 2086  134   10 6713    3   31   15   12   15   18 1224   20   73\n",
      "   22    0    0    0]\n",
      "  Ground word : SOS 我会 逼 自己 做到 EOS\n",
      "    Output    : 我 你 你 知道\n",
      "\n",
      "  Input word  : SOS 继续 做 你 唯一 的 城堡 EOS n d a n d a NOP ao NOE 6 NOR\n",
      "  Input index : [ 1406   885     5   955    10 13437     3    31    17    40    31    17\n",
      "    40    18  1224    20    33    22]\n",
      "  Ground word : SOS 习惯 就 好 习惯 就 好 EOS\n",
      "    Output    : 世界 世界 不 就 不 好\n",
      "\n",
      "Max length : 17\n",
      "  Input word  : SOS 眼眶 也 是 红 的 EOS v r t p r v NOP uo NOE 6 NOR\n",
      "  Input index : [1010   68   96 1256   10    3   15   12   92   13   12   15   18  461\n",
      "   20   33   22]\n",
      "  Ground word : SOS 想起 你 昨天 对 我 说 EOS\n",
      "    Output    : 我 我 我 对 你 说\n",
      "\n",
      "  Input word  : SOS 给 你 一生 永恒 的 承诺 EOS c a n NOP ing NOE 3 NOR\n",
      "  Input index : [ 568    5  288  755   10 2606    3   64   40   31   18   65   20   83\n",
      "   22    0    0]\n",
      "  Ground word : SOS 如果 爱似 流星 EOS\n",
      "    Output    : 哪怕 如此 平静\n",
      "\n",
      "  Input word  : SOS 相信 什么 道理 EOS v r t NOP u NOE 3 NOR\n",
      "  Input index : [ 94 135 219   3  15  12  92  18  19  20  83  22   0   0   0   0   0]\n",
      "  Ground word : SOS 回忆 什么 过去 EOS\n",
      "    Output    : 我们 你 过去\n",
      "\n",
      "Max length : 22\n",
      "  Input word  : SOS 谁 说 我 的 目光 流淌 不成 河 EOS v r v l v n NOP ai NOE 6 NOR\n",
      "  Input index : [ 254  134   36   10 3639 7028 7398 3296    3   15   12   15   63   15\n",
      "   31   18  206   20   33   22    0    0]\n",
      "  Ground word : SOS 陪 我 到 可可西里 看一看 海 EOS\n",
      "    Output    : 谁 你 可可西里 可可西里 看一看 海\n",
      "\n",
      "  Input word  : SOS 我会 被 埋 在 故乡 的 土里 EOS v m t uj n v r uj n NOP ong NOE 9 NOR\n",
      "  Input index : [  961    46   768    24  5565    10 12700     3    15    14    92    41\n",
      "    31    15    12    41    31    18   439    20    21    22]\n",
      "  Ground word : SOS 让 一群 生前 的 人 忘却 我 的 面孔 EOS\n",
      "    Output    : 我 的 的 的 的 的 的 的 天空\n",
      "\n",
      "  Input word  : SOS 谭咏 麟 李克勤 北京 祝福 你 祝福 你 EOS nr nr n v zg m n NOP i NOE 7 NOR\n",
      "  Input index : [4977 4978 6180 4023  451    5  451    5    3   39   39   31   15  208\n",
      "   14   31   18   32   20   66   22    0]\n",
      "  Ground word : SOS 吴辰君 陈 小朵 激励 每 一天 传奇 EOS\n",
      "    Output    : 吴辰君 陈 小朵 激励 激励 激励 传奇\n",
      "\n",
      "Max length : 21\n",
      "  Input word  : SOS 明天 可否 会 变 晴朗 EOS c v d t NOP ian NOE 4 NOR\n",
      "  Input index : [  98 1163  197  371 4188    3   64   15   17   92   18  124   20   73\n",
      "   22    0    0    0    0    0    0]\n",
      "  Ground word : SOS 或者 猜不透 已 明天 EOS\n",
      "    Output    : 不管 和 回到 从前\n",
      "\n",
      "  Input word  : SOS 你 是 我 在 远方 的 爱人 EOS r p v r d a uj ns NOP en NOE 8 NOR\n",
      "  Input index : [   5   96   36   24 1395   10 1285    3   12   13   15   12   17   40\n",
      "   41  131   18  694   20   42   22]\n",
      "  Ground word : SOS 你 在 离 我 很 远 的 城镇 EOS\n",
      "    Output    : 我 你 的 的 我 我 的 深\n",
      "\n",
      "  Input word  : SOS 仍 在 催促 的 是 你 呼喊 过 的 劲 EOS v r m p v NOP in NOE 5 NOR\n",
      "  Input index : [ 341   24 3265   10   96    5 6491  245   10 3774    3   15   12   14\n",
      "   13   15   18   86   20   79   22]\n",
      "  Ground word : SOS 留下 我 一个 在 拼 EOS\n",
      "    Output    : 我 我 一生 一起 抱紧\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    evaluate_batch(sess,_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = pickle.load(open(os.path.join(log_path,\"loss.pkl\"),\"rb\"))\n",
    "## 16*5085 is iteration count\n",
    "loss_list = loss_list[0:22*5085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0 loss :    2.8592 time :    1.94\n",
      "Max length : 21\n",
      "  Input word  : ['SOS', '参天大树', '下', '隔壁家', '姐姐', '到哪去', '了', 'EOS', 'n', 'f', 'r', 'd', 'v', 'ul', 's', 'uj', 'ns', 'NOP', 'a', 'NOE', '9', 'NOR']\n",
      "  Input index : [56616  1767 48902 13811 32692    52     3    31    16    12    17    15\n",
      "    50    72    41   131    18    40    20    21    22]\n",
      "  Ground word : ['SOS', '参天大树', '下', '他', '已', '卖掉', '了', '手中', '的', '吉他', 'EOS']\n",
      "    Output    : ['参天大树', '参天大树', '下', '山里', '了', '了', '我', '的', '吉他']\n",
      "\n",
      "  Input word  : ['SOS', '难道', '爱神', '在', '感冒', 'EOS', 'n', 'd', 'r', 'NOP', 'iao', 'NOE', '3', 'NOR']\n",
      "  Input index : [  719 18617    24  4108     3    31    17    12    18   312    20    83\n",
      "    22     0     0     0     0     0     0     0     0]\n",
      "  Ground word : ['SOS', '地球', '每一分', '每一秒', 'EOS']\n",
      "    Output    : ['我', '每一分', '每一秒']\n",
      "\n",
      "  Input word  : ['SOS', '给', '坚强', '一个', '机会', 'EOS', 'd', 'm', 'uj', 'n', 'NOP', 'ui', 'NOE', '4', 'NOR']\n",
      "  Input index : [ 568 4722  127 2193    3   17   14   41   31   18  279   20   73   22\n",
      "    0    0    0    0    0    0    0]\n",
      "  Ground word : ['SOS', '再', '多', '的', '泪水', 'EOS']\n",
      "    Output    : ['再', '的', '的', '机会']\n",
      "\n",
      "Interrupt .. Save lastest model...  141.77\n",
      "\n",
      "Save loss history...\n",
      "\n",
      "### Training Finished!! ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = train_batch_size\n",
    "n_step = idx_in_sen.shape[0]//batch_size\n",
    "\n",
    "r_index = np.arange(idx_in_sen.shape[0])\n",
    "# loss_list = []\n",
    "try:\n",
    "    for e in range(23,40+1):\n",
    "        np.random.shuffle(r_index)\n",
    "        start_time = time.time()\n",
    "        start = 0\n",
    "        for s in range(n_step):\n",
    "            idx = r_index[start:start+batch_size]\n",
    "            _,l = sess.run([_update,_loss] , feed_dict=get_batch(idx))\n",
    "            start += batch_size\n",
    "            print(\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\".format(s,l,time.time()-start_time) , end=\"\\r\")\n",
    "            loss_list.append(l)\n",
    "            if s % print_interval == 0:\n",
    "                print(\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\".format(s,l,time.time()-start_time) , end=\"\\n\")\n",
    "                evaluate_batch(sess,_pred,3)\n",
    "        print(\"\\nEpoch {0:>3d}/{1:d} loss : {2:>9.4f} time : {3:>8.2f}\".format(e,n_epoch,l,time.time()-start_time))\n",
    "\n",
    "        evaluate_batch(sess,_pred,3)\n",
    "\n",
    "        if e%2 == 0:\n",
    "            saver.save(sess,os.path.join(model_path,\"model_{}.ckpt\".format(e)))\n",
    "except KeyboardInterrupt :\n",
    "    print(\"Interrupt .. Save lastest model...\")\n",
    "    saver.save(sess,os.path.join(model_path,\"model_{}.ckpt\".format(\"lastest\")))\n",
    "    print()\n",
    "    \n",
    "print(\"Save loss history...\")\n",
    "pickle.dump(loss_list,open(os.path.join(log_path,\"loss.pkl\") , \"wb\"))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(\"\\n### Training Finished!! ###\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.12",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
