{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as ly\n",
    "sess_opt = tf.ConfigProto(gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95 , allow_growth=True)\n",
    "                         ,device_count={'GPU': 1})\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import exist_or_mkdir , data_manager , transform_orig\n",
    "\n",
    "exp_folder = \"No_attn_ver1\"\n",
    "model_path = \"model_para\"\n",
    "tmp_path = \"tmp\"\n",
    "log_path = \"log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path : './No_attn_ver1'\n",
      "Path : './No_attn_ver1/model_para'\n",
      "Path : './No_attn_ver1/tmp'\n",
      "Path : './No_attn_ver1/log'\n"
     ]
    }
   ],
   "source": [
    "exp_folder = exist_or_mkdir(\"./\",exp_folder)\n",
    "model_path = exist_or_mkdir(exp_folder,model_path)\n",
    "tmp_path = exist_or_mkdir(exp_folder,tmp_path)\n",
    "log_path = exist_or_mkdir(exp_folder,log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder_max_len = 50\n",
    "Decoder_max_len = 30\n",
    "min_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading Train Data ###\n",
      "Data count : 651339\n",
      "\n",
      "### Data view ###\n",
      "Original data  : ['SOS', '當', '我', '還', '是', '一個', '小', '娃娃', '媽媽', '經常', '對', '我們', '說', '話', 'EOS', 'n', 'r', 'v', 'm', 'n', 'a', 'r', 'q', 'n', 'r', 'df', 'v', 'r', 'NOP', 'a', 'NOE', '13', 'NOR']\n",
      "Input sentence : ['SOS', '當', '我', '還', '是', '一個', '小', '娃娃', '媽媽', '經常', '對', '我們', '說', '話', 'EOS']\n",
      "Gramma         : ['n', 'r', 'v', 'm', 'n', 'a', 'r', 'q', 'n', 'r', 'df', 'v', 'r']\n",
      "Length         : 13\n",
      "Rhyme          : a\n",
      "Output Sentence : ['SOS', '孩子', '你', '有', '一天', '會長', '大', '這', '句', '話', '你', '不要', '忘記', '它', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "print(\"### Loading Train Data ###\")\n",
    "data_agent = data_manager(\"data/all/train.csv\" , train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading Test Data ###\n",
      "Data count : 70000\n",
      "\n",
      "### Data view ###\n",
      "Original data  : ['SOS', '放过', '自己', 'EOS', 'df', 'n', 'p', 'v', 'f', 'NOP', 'i', 'NOE', '5', 'NOR']\n",
      "Input sentence : ['SOS', '放过', '自己', 'EOS']\n",
      "Gramma         : ['df', 'n', 'p', 'v', 'f']\n",
      "Length         : 5\n",
      "Rhyme          : i\n"
     ]
    }
   ],
   "source": [
    "print(\"### Loading Test Data ###\")\n",
    "test_agent = data_manager(\"data/all/test.csv\" , train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Count : 3\n",
      "Max Length : [50, 30]\n",
      "Word Count : 59465\n",
      "Orig data  : ['SOS', '扫', '过', '远方', '的', '国度', '夹带', '了', '爱意', '再', '吹', '到岸', 'EOS', 'd', 'v', 'v', 'r', 'v', 'p', 'n', 'v', 'r', 'NOP', 'o', 'NOE', '9', 'NOR']\n",
      "Index data : [13047, 245, 1395, 10, 8710, 5605, 52, 2676, 432, 1006, 39319, 3, 17, 15, 15, 12, 15, 13, 31, 15, 12, 18, 150, 20, 21, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Output Orig data  : ['SOS', '仍然', '相信', '预觉', '你', '会', '以', '双手', '抚慰', '我', 'EOS']\n",
      "Output Index data : [2, 1632, 94, 39320, 5, 197, 477, 2677, 6405, 36, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "idx_in_sen , idx_out_sen , mask_in , mask_out , length_in , idx2word , word2idx , remain_idx = \\\n",
    "    transform_orig([data_agent.orig_data,data_agent.out_sen],min_count=min_count,\n",
    "                   max_len = [Encoder_max_len,Decoder_max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((651130, 50), (651130, 30), (651130, 50), (651130, 30), (651130,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_in_sen.shape , idx_out_sen.shape , mask_in.shape , mask_out.shape , length_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({\"orig_word\":[idx2word,word2idx] },\n",
    "            open(os.path.join(tmp_path,\"tokenizer.pkl\") , \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(inputs , dim , name , init_state=None , t_len=20 , reuse=False , stack_flag=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(dim,name=name,reuse=reuse)\n",
    "    if init_state:\n",
    "        state = init_state\n",
    "    else:\n",
    "        state = [tf.zeros([tf.shape(_in)[0] , cell.state_size[0]]),\n",
    "                 tf.zeros([tf.shape(_in)[0] , cell.state_size[1]])]\n",
    "    output_seq = []\n",
    "    for t in range(t_len):\n",
    "        if stack_flag:\n",
    "            out , state = cell(inputs[:,t] , state)\n",
    "        else:\n",
    "            out , state = cell(inputs[t] , state)\n",
    "        output_seq.append(out)\n",
    "    \n",
    "    return output_seq , state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attend_vector(inputs , state , mask , name):\n",
    "    with tf.name_scope(\"Attention\"):\n",
    "        state = tf.tile(tf.expand_dims(state , axis=1) , [1,tf.shape(inputs)[1],1])\n",
    "        concat_vec = tf.concat([inputs,state],axis=-1)\n",
    "        fc1 = ly.fully_connected(concat_vec,128,activation_fn=tf.nn.leaky_relu,biases_initializer=None,\n",
    "                                 scope=\"Attn_{}_1\".format(name),reuse=tf.AUTO_REUSE)\n",
    "        fc2 = ly.fully_connected(fc1,64,activation_fn=tf.nn.leaky_relu,biases_initializer=None,\n",
    "                                 scope=\"Attn_{}_2\".format(name),reuse=tf.AUTO_REUSE)\n",
    "        fc3 = ly.fully_connected(fc1,1,activation_fn=None,biases_initializer=None,\n",
    "                                 scope=\"Attn_{}_3\".format(name),reuse=tf.AUTO_REUSE)\n",
    "        score = tf.nn.softmax(fc3*mask , axis=1) ## wrong\n",
    "        ## define my softmax\n",
    "#         exp_fc3 = tf.exp(fc3)*mask\n",
    "#         exp_sum = tf.reduce_sum(exp_fc3,axis=1,keepdims=True)\n",
    "#         score = exp_fc3/exp_sum\n",
    "    \n",
    "    return score , tf.reduce_sum(inputs*score , axis=1)\n",
    "\n",
    "def attn_Encoder(inputs , mask , dim , name , init_state=None , t_len=20 , reuse=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(dim,name=name,reuse=reuse)\n",
    "    if init_state:\n",
    "        state = init_state\n",
    "    else:\n",
    "        state = [tf.zeros([tf.shape(_in)[0] , cell.state_size[0]]),\n",
    "                 tf.zeros([tf.shape(_in)[0] , cell.state_size[1]])]\n",
    "    output_seq = []\n",
    "    score_seq = []\n",
    "    for t in range(t_len):\n",
    "        score , attn_vec = attend_vector(inputs,state[1],mask,name=\"Encode\")\n",
    "        out , state = cell(attn_vec,state)\n",
    "        output_seq.append(out)\n",
    "        score_seq.append(score)\n",
    "    \n",
    "    return output_seq , state , score_seq \n",
    "\n",
    "\n",
    "def attn_Decoder(inputs , inputs_E , mask , dim , name , init_state=None , t_len=20 , reuse=False , stack_flag=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(dim,name=name,reuse=reuse)\n",
    "    if init_state:\n",
    "        state = init_state\n",
    "    else:\n",
    "        state = [tf.zeros([tf.shape(_in)[0] , cell.state_size[0]]),\n",
    "                 tf.zeros([tf.shape(_in)[0] , cell.state_size[1]])]\n",
    "    output_seq = []\n",
    "    score_seq = []\n",
    "    for t in range(t_len):\n",
    "        score , attn_vec = attend_vector(inputs_E,state[1],mask,name=\"Decode\")\n",
    "        if stack_flag:\n",
    "            attn_vec = tf.concat([attn_vec,inputs[:,t]] , axis=-1)\n",
    "        else:\n",
    "            attn_vec = tf.concat([attn_vec,inputs[t]] , axis=-1)\n",
    "        out , state = cell(attn_vec,state)\n",
    "        output_seq.append(out)\n",
    "        score_seq.append(score)\n",
    "    \n",
    "    return output_seq , state , score_seq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_clf(inputs,dim,embd):\n",
    "    fc1 = ly.fully_connected(inputs,dim,activation_fn=tf.nn.leaky_relu,scope=\"clf_fc1\",reuse=tf.AUTO_REUSE)\n",
    "    fc2 = ly.fully_connected(fc1,int(embd.shape[0]),activation_fn=None,scope=\"clf_fc2\",reuse=tf.AUTO_REUSE)\n",
    "    return fc2@embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_catece(x):\n",
    "    logit = x[0]\n",
    "    idx = x[1]\n",
    "    ce = []\n",
    "    for t in range(Decoder_max_len-1):\n",
    "        ce.append( tf.log(tf.nn.embedding_lookup(logit[t],idx[t])+1e-10) )\n",
    "    return tf.stack(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Building!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Seq_g = tf.Graph()\n",
    "embd_dim = 128\n",
    "clf_dim = 256\n",
    "RNN_L0_dim = 256\n",
    "RNN_L1_dim = 256\n",
    "RNN_L2_dim = 384\n",
    "with Seq_g.as_default():\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        _in = tf.placeholder(tf.int32,[None,None])\n",
    "        _in_mask = tf.placeholder(tf.float32,[None,None])\n",
    "        in_mask = tf.expand_dims(_in_mask,axis=-1)\n",
    "        \n",
    "        _in_length = tf.placeholder(tf.int32,[None])\n",
    "        \n",
    "        _out = tf.placeholder(tf.int32,[None,Decoder_max_len])\n",
    "        _out_mask = tf.placeholder(tf.float32,[None,Decoder_max_len])\n",
    "        gt = _out[:,1::]\n",
    "        gt_mask = _out_mask[:,1::]\n",
    "        \n",
    "        schedual_rate = tf.random_uniform([Decoder_max_len],maxval=1.0)\n",
    "        schedual_th = tf.placeholder(tf.float32)\n",
    "        infer_start = tf.ones([tf.shape(_in)[0]],dtype=tf.int32)\n",
    "        \n",
    "    with tf.name_scope(\"Embedding\"):\n",
    "        ## word embedding\n",
    "        _embd = tf.Variable(tf.truncated_normal([len(idx2word) , embd_dim],stddev=0.1),name=\"Word_Embd\")\n",
    "        _embd_T = tf.transpose(_embd,[1,0])\n",
    "        x_vector = tf.nn.embedding_lookup(_embd,_in,max_norm=5)\n",
    "        y_vector = tf.nn.embedding_lookup(_embd,_out,max_norm=5)\n",
    "        \n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"Encoder\"):\n",
    "        e_cell0 = tf.contrib.rnn.LSTMCell(RNN_L0_dim,name=\"E_layer_0\",reuse=False)\n",
    "        e_cell1 = tf.contrib.rnn.LSTMCell(RNN_L1_dim,name=\"E_layer_1\",reuse=False)\n",
    "        \n",
    "        E_layer_0 , E_state_0= tf.nn.dynamic_rnn(e_cell0,x_vector,sequence_length=_in_length,dtype=tf.float32)\n",
    "        E_layer_1 , E_state_1= tf.nn.dynamic_rnn(e_cell1,E_layer_0,sequence_length=_in_length,dtype=tf.float32)\n",
    "        \n",
    "    with tf.name_scope(\"Decoder\"):\n",
    "        \n",
    "        D_layer_0 , D_state_0 = Encoder(y_vector,RNN_L0_dim,\"rnn/E_layer_0\",init_state=E_state_0,reuse=True,\n",
    "                                        t_len=Decoder_max_len-1,stack_flag=True)\n",
    "#         D_layer_1 , D_state_1 , D_score = attn_Decoder(D_layer_0,E_layer_1,in_mask,300,name=\"Attn_D_layer_1\",\n",
    "#                                                        init_state=E_state_1,t_len=Decoder_max_len-1,stack_flag=False)\n",
    "        D_layer_1 , D_state_1 = Encoder(D_layer_0,RNN_L1_dim,\"rnn/E_layer_1\",init_state=E_state_1,reuse=True,\n",
    "                                        t_len=Decoder_max_len-1,stack_flag=False)\n",
    "        D_layer_2 , D_state_2 = Encoder(D_layer_1,RNN_L2_dim,\"D_layer_2\",reuse=False,\n",
    "                                        t_len=Decoder_max_len-1,stack_flag=False)\n",
    "        \n",
    "        output_seq = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            choice_input = D_layer_2[t]\n",
    "            out = word_clf(choice_input,clf_dim,_embd_T)\n",
    "            output_seq.append(out)\n",
    "        _logits = tf.stack(output_seq,axis=1)\n",
    "        _prob = tf.nn.softmax(_logits,axis=-1)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"Loss\"):\n",
    "#         cross_entropy_0 = tf.map_fn(mask_catece,elems=(_prob,gt),dtype=tf.float32)\n",
    "#         cross_entropy = tf.reduce_sum(cross_entropy_0*gt_mask,axis=-1)/tf.reduce_sum(gt_mask,axis=-1)\n",
    "#         _loss = -tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        gt = tf.one_hot(gt,depth=len(idx2word),dtype=tf.float32)\n",
    "        cross_entropy_0 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(gt,[-1,len(idx2word)]),\n",
    "                                                                     logits=tf.reshape(_logits,[-1,len(idx2word)]))\n",
    "        cross_entropy_1 = tf.reshape(cross_entropy_0,[-1,Decoder_max_len-1])\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy_1*gt_mask,axis=-1)/tf.reduce_sum(gt_mask,axis=-1)\n",
    "        _loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "    with tf.name_scope(\"Train_strategy\"):\n",
    "        opt = tf.train.AdamOptimizer(1e-4)\n",
    "        _update = opt.minimize(_loss)\n",
    "    \n",
    "    with tf.name_scope(\"Inference\"):\n",
    "        ## start at Encoder layer 2 : E_layer2\n",
    "        infer_out = tf.nn.embedding_lookup(_embd,infer_start)\n",
    "        infer_state_0 = E_state_0\n",
    "        infer_state_1 = E_state_1\n",
    "        \n",
    "        infer_score_seq = []\n",
    "        infer_pred_idx_seq = []\n",
    "        infer_logits_seq = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            tmp = Encoder([infer_out],RNN_L0_dim,\"rnn/E_layer_0\",init_state=infer_state_0,reuse=True,\n",
    "                          t_len=1,stack_flag=False)\n",
    "            infer_layer_0 , infer_state_0 = tmp\n",
    "            \n",
    "#             tmp = attn_Decoder(infer_layer_0,E_layer_1,in_mask,256,name=\"Attn_D_layer_1\",\n",
    "#                                init_state=infer_state_1,t_len=1,reuse=True,stack_flag=False)\n",
    "#             infer_layer_1 , infer_state_1 , infer_score = tmp\n",
    "            tmp = Encoder(infer_layer_0,RNN_L1_dim,\"rnn/E_layer_1\",init_state=infer_state_1,reuse=True,\n",
    "                          t_len=1,stack_flag=False)\n",
    "            infer_layer_1 , infer_state_1 = tmp\n",
    "            infer_layer_2 , infer_state_2 = Encoder(infer_layer_1,RNN_L2_dim,\"D_layer_2\",reuse=True,t_len=1,stack_flag=False)\n",
    "            \n",
    "            \n",
    "#             infer_score_seq.append(infer_score)\n",
    "            \n",
    "            infer_out = word_clf(infer_layer_2[0],clf_dim,_embd_T)\n",
    "            infer_logits_seq.append(infer_out)\n",
    "            \n",
    "            out_index = tf.argmax(infer_out,axis=1)\n",
    "            infer_pred_idx_seq.append(out_index)\n",
    "            infer_out = tf.nn.embedding_lookup(_embd , out_index)\n",
    "            \n",
    "        infer_pred_idx_seq = tf.stack(infer_pred_idx_seq,axis=1)\n",
    "        infer_logits = tf.stack(infer_logits_seq,axis=1)\n",
    "        infer_prob = tf.nn.softmax(infer_logits,axis=-1)\n",
    "    \n",
    "    tf.summary.FileWriter(log_path,graph=Seq_g)\n",
    "    _init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=10,var_list=tf.global_variables())\n",
    "    \n",
    "print(\"Finish Building!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start Training ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"### Start Training ###\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=Seq_g,config=sess_opt)\n",
    "sess.run(_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(i):\n",
    "    tmp_end = max(length_in[i])\n",
    "    my_dict = {\n",
    "        _in:idx_in_sen[i,:tmp_end],\n",
    "        _in_mask:mask_in[i,:tmp_end],\n",
    "        _out:idx_out_sen[i],\n",
    "        _out_mask:mask_out[i],\n",
    "        _in_length:length_in[i]\n",
    "    }\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(sess,_pred,count=3):\n",
    "    idx = np.random.choice(idx_in_sen.shape[0],[count])\n",
    "    tmp_max_len = max(length_in[idx])\n",
    "    my_dict = {\n",
    "        _in:idx_in_sen[idx,:tmp_max_len],\n",
    "        _in_mask:mask_in[idx,:tmp_max_len],\n",
    "        _in_length:length_in[idx]\n",
    "    }\n",
    "    pred = sess.run(_pred , feed_dict=my_dict)\n",
    "    \n",
    "    word_seq = []\n",
    "    for i in range(3):\n",
    "        idx_sen = pred[i]\n",
    "        tmp = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            if(idx_sen[t] == 3):\n",
    "                break\n",
    "            tmp.append(idx2word[idx_sen[t]])\n",
    "        word_seq.append(tmp)\n",
    "    \n",
    "    print(\"Max length :\" , tmp_max_len)\n",
    "    for i in range(3):\n",
    "        print(\"  Input word  :\" , data_agent.orig_data[remain_idx[idx[i]]])\n",
    "        print(\"  Input index :\" , idx_in_sen[idx[i],:tmp_max_len])\n",
    "        print(\"  Ground word :\" , data_agent.out_sen[remain_idx[idx[i]]])\n",
    "        print(\"    Output    :\" , word_seq[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,59465] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: Embedding/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Embedding/Word_Embd/read, Train_strategy/gradients/Embedding/transpose_grad/InvertPermutation)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Encoder/rnn/stack/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_463_Encoder/rnn/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'Embedding/transpose', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 359, in dispatch_queue\n    yield self.process_one()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-b1599a94c0d7>\", line 27, in <module>\n    _embd_T = tf.transpose(_embd,[1,0])\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1484, in transpose\n    ret = transpose_fn(a, perm, name=name)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8552, in transpose\n    \"Transpose\", x=x, perm=perm, name=name)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,59465] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: Embedding/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Embedding/Word_Embd/read, Train_strategy/gradients/Embedding/transpose_grad/InvertPermutation)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Encoder/rnn/stack/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_463_Encoder/rnn/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,59465] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: Embedding/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Embedding/Word_Embd/read, Train_strategy/gradients/Embedding/transpose_grad/InvertPermutation)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Encoder/rnn/stack/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_463_Encoder/rnn/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-053a0a94acf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,59465] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: Embedding/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Embedding/Word_Embd/read, Train_strategy/gradients/Embedding/transpose_grad/InvertPermutation)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Encoder/rnn/stack/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_463_Encoder/rnn/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'Embedding/transpose', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/local/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/usr/local/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 359, in dispatch_queue\n    yield self.process_one()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-b1599a94c0d7>\", line 27, in <module>\n    _embd_T = tf.transpose(_embd,[1,0])\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1484, in transpose\n    ret = transpose_fn(a, perm, name=name)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8552, in transpose\n    \"Transpose\", x=x, perm=perm, name=name)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/derricksu/Envs/DSML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,59465] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: Embedding/transpose = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Embedding/Word_Embd/read, Train_strategy/gradients/Embedding/transpose_grad/InvertPermutation)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: Encoder/rnn/stack/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_463_Encoder/rnn/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "batch_size = 300\n",
    "n_epoch = 100\n",
    "n_step = idx_in_sen.shape[0]//batch_size\n",
    "\n",
    "r_index = np.arange(idx_in_sen.shape[0])\n",
    "loss_list = []\n",
    "\n",
    "for e in range(1,n_epoch+1):\n",
    "    np.random.shuffle(r_index)\n",
    "    start_time = time.time()\n",
    "    start = 0\n",
    "    for s in range(n_step):\n",
    "        idx = r_index[start:start+batch_size]\n",
    "        _,l = sess.run([_update,_loss] , feed_dict=get_batch(idx))\n",
    "        start += batch_size\n",
    "        print(\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\".format(s,l,time.time()-start_time) , end=\"\\r\")\n",
    "        if s % 500 == 0:\n",
    "            print(\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\".format(s,l,time.time()-start_time) , end=\"\\n\")\n",
    "            evaluate_batch(sess,infer_pred_idx_seq,3)\n",
    "    \n",
    "    loss_list.append(l)\n",
    "    print(\"\\nEpoch {0:>3d}/{1:d} loss : {2:>9.4f} time : {3:>8.2f}\".format(e,n_epoch,l,time.time()-start_time))\n",
    "    \n",
    "    evaluate_batch(sess,infer_pred_idx_seq,3)\n",
    "    \n",
    "    if e%4 == 0:\n",
    "        saver.save(sess,os.path.join(model_path,\"model_{}.ckpt\".format(e)))\n",
    "        pickle.dump(loss_list,open(os.path.join(log_path,\"loss.pkl\") , \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Training Finished!! ###\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'OOV', 'SOS', 'EOS']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_test_data(sess,_pred,x,word2idx,batch_size=1000):\n",
    "    start = 0\n",
    "    pred_word_seq = []\n",
    "    while(start < len(x)):\n",
    "        batch_idx_sen = []\n",
    "        batch_length = []\n",
    "        batch_mask = []\n",
    "        max_len = 0\n",
    "        for s in x[start : start+batch_size]:\n",
    "            l = len(s)-1\n",
    "            if(l>max_len):\n",
    "                max_len = l\n",
    "        \n",
    "        for s in x[start : start+batch_size]:\n",
    "            arr = []\n",
    "            batch_mask.append( np.zeros([max_len]))\n",
    "            batch_mask[-1][0:len(s[1::])] += 1\n",
    "            batch_length.append(len(s[1::]))\n",
    "            for ss in s[1::]:\n",
    "                try:\n",
    "                    arr.append(word2idx[ss])\n",
    "                except:\n",
    "                    arr.append(1)\n",
    "            arr.extend([0]*(max_len-len(s[1::])))\n",
    "            batch_idx_sen.append(arr)\n",
    "        batch_idx_sen = np.array(batch_idx_sen)\n",
    "        batch_length = np.array(batch_length)\n",
    "        batch_mask = np.stack(batch_mask)\n",
    "        \n",
    "        pred_sen = sess.run(_pred,feed_dict={\n",
    "            _in:batch_idx_sen,\n",
    "            _in_length:batch_length,\n",
    "            _in_mask:batch_mask\n",
    "        })\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            idx_sen = pred_sen[i]\n",
    "            tmp = []\n",
    "            for t in range(Decoder_max_len-1):\n",
    "                if(idx_sen[t] == 3):\n",
    "                    break\n",
    "                elif(idx_sen[t] == 1):\n",
    "                    tmp.append(np.random.choice(idx2word))\n",
    "                else:\n",
    "                    tmp.append(idx2word[idx_sen[t]])\n",
    "            pred_word_seq.append(\" \".join(tmp))\n",
    "        start += batch_size\n",
    "        \n",
    "    return pred_word_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_infer = infer_test_data(sess,infer_pred_idx_seq,test_agent.orig_data,word2idx,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  SOS 夜晚 有 灯光 EOS v d v a NOP iang NOE 4 NOR\n",
      "Infer :  欲望 都 变得 明亮\n",
      "\n",
      "Input :  SOS 苍天 不解 恨怨 痴心 爱侣 仍 难 如愿 EOS v zg v v c d a m t NOP ian NOE 9 NOR\n",
      "Infer :  忘掉 反 爱 爱 但 更 差 多少 明天\n",
      "\n",
      "Input :  SOS 要不是 有 情郎 跟 我 要 分开 EOS r n v v v NOP ai NOE 5 NOR\n",
      "Infer :  我 眼泪 不会 掉下来 掉下来\n",
      "\n",
      "Input :  SOS 他 的 手掌 有种 粗糙 的 体贴 EOS r p r v n v s NOP ian NOE 7 NOR\n",
      "Infer :  我 把 你 放在 眼睛 看着 窗前\n",
      "\n",
      "Input :  SOS 恋恋 恋 EOS r v i NOP uan NOE 3 NOR\n",
      "Infer :  谁 去 袖手旁观\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(len(test_agent.orig_data) , 5 , replace=False):\n",
    "    print(\"Input : \" , \" \".join(test_agent.orig_data[i]))\n",
    "    print(\"Infer : \" , test_infer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_infer(data,name):\n",
    "    path = os.path.join(exp_folder,name)\n",
    "    print(\"Save at '{}'\".format( path))\n",
    "    with open( path, \"w\") as f:\n",
    "        for s in data:\n",
    "            s = \"\".join(s.split())\n",
    "            if(len(s) == 0):\n",
    "                s = np.random.choice(idx2word[4::])\n",
    "            f.write(s+\"\\n\")\n",
    "\n",
    "def save_infer_seg(data,name):\n",
    "    path = os.path.join(exp_folder,name)\n",
    "    print(\"Save at '{}'\".format( path))\n",
    "    with open( path, \"w\") as f:\n",
    "        for s in data:\n",
    "            if(len(s) == 0):\n",
    "                s = np.random.choice(idx2word[4::])\n",
    "            f.write(s+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_infer(test_infer,\"infer_output.txt\")\n",
    "save_infer_seg(test_infer,\"infer_seg.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "dsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
