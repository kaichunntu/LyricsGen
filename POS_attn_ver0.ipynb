{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as ly\n",
    "sess_opt = tf.ConfigProto(gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95 , allow_growth=True)\n",
    "                         ,device_count={'GPU': 1})\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import exist_or_mkdir , data_manager , transform_orig\n",
    "\n",
    "exp_folder = \"POS_attn_ver0\"\n",
    "model_path = \"model_para\"\n",
    "tmp_path = \"tmp\"\n",
    "log_path = \"log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path : './POS_attn_ver0'\n",
      "Path : './POS_attn_ver0/model_para'\n",
      "Path : './POS_attn_ver0/tmp'\n",
      "Path : './POS_attn_ver0/log'\n"
     ]
    }
   ],
   "source": [
    "exp_folder = exist_or_mkdir(\"./\",exp_folder)\n",
    "model_path = exist_or_mkdir(exp_folder,model_path)\n",
    "tmp_path = exist_or_mkdir(exp_folder,tmp_path)\n",
    "log_path = exist_or_mkdir(exp_folder,log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder_max_len = 60\n",
    "Decoder_max_len = 30\n",
    "min_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = [\"data/{}/train.csv\".format(x) for x in [\"pos\"]]\n",
    "test_path = [\"data/{}/test.csv\".format(x) for x in [\"pos\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading Train Data ###\n",
      "Data count : 651339\n",
      "\n",
      "### Data view ###\n",
      "Original data  : ['SOS', '我们', '都', '是', '一颗颗', '小', '螺丝钉', 'EOS', 'r', 'd', 'v', 'v', 'a', 'r', 'n', 'uj', 'n', 'n', 'NOP']\n",
      "Output Sentence : ['SOS', '你', '就', '去', '扮演', '好', '那', '一颗颗', '的', '小', '螺丝钉', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "print(\"### Loading Train Data ###\")\n",
    "data_agent = data_manager(train_path , train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading Test Data ###\n",
      "Data count : 70000\n",
      "\n",
      "### Data view ###\n",
      "Original data  : ['SOS', '你', '给', '了', '我', '无限量', '的', '呵护', 'EOS', 'p', 'r', 'i', 'v', 'v', 'NOP']\n"
     ]
    }
   ],
   "source": [
    "print(\"### Loading Test Data ###\")\n",
    "test_agent = data_manager(test_path , train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Count : 3\n",
      "Max Length : [60, 30]\n",
      "Use pre-trained tokenizer\n",
      "Word Count : 59465\n",
      "Orig data  : ['SOS', '悲伤', '是', '奢侈品', '我', '消受', '不起', 'EOS', 'a', 'v', 'n', 'd', 'a', 'NOP']\n",
      "Index data : [1404, 96, 15778, 36, 15779, 1816, 3, 40, 15, 31, 17, 40, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Output Orig data  : ['SOS', '快乐', '像', '噩梦', '一转眼', '惊醒', 'EOS']\n",
      "Output Index data : [2, 188, 178, 6417, 1494, 7123, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "idx_in_sen , idx_out_sen , mask_in , mask_out , length_in , idx2word , word2idx , remain_idx = \\\n",
    "    transform_orig([data_agent.orig_data,data_agent.out_sen],min_count=min_count,\n",
    "                   max_len = [Encoder_max_len,Decoder_max_len],path=\"Attn_ver1/tmp/tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({\"orig_word\":[idx2word,word2idx] },\n",
    "            open(os.path.join(tmp_path,\"tokenizer.pkl\") , \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(inputs , dim , name , init_state=None , t_len=20 , reuse=False , stack_flag=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(dim,name=name,reuse=reuse)\n",
    "    if init_state:\n",
    "        state = init_state\n",
    "    else:\n",
    "        state = [tf.zeros([tf.shape(inputs)[0] , cell.state_size[0]]),\n",
    "                 tf.zeros([tf.shape(inputs)[0] , cell.state_size[1]])]\n",
    "    output_seq = []\n",
    "    for t in range(t_len):\n",
    "        if stack_flag:\n",
    "            out , state = cell(inputs[:,t] , state)\n",
    "        else:\n",
    "            out , state = cell(inputs[t] , state)\n",
    "        output_seq.append(out)\n",
    "    \n",
    "    return output_seq , state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attend_vector(inputs , state , mask , name):\n",
    "    with tf.name_scope(\"Attention\"):\n",
    "        state = tf.tile(tf.expand_dims(state , axis=1) , [1,tf.shape(inputs)[1],1])\n",
    "        concat_vec = tf.concat([inputs,state],axis=-1)\n",
    "        fc1 = ly.fully_connected(concat_vec,256,activation_fn=tf.nn.leaky_relu,biases_initializer=None,\n",
    "                                 scope=\"Attn_{}_1\".format(name),reuse=tf.AUTO_REUSE)\n",
    "        fc2 = ly.fully_connected(fc1,128,activation_fn=tf.nn.leaky_relu,biases_initializer=None,\n",
    "                                 scope=\"Attn_{}_2\".format(name),reuse=tf.AUTO_REUSE)\n",
    "        fc3 = ly.fully_connected(fc1,1,activation_fn=None,biases_initializer=None,\n",
    "                                 scope=\"Attn_{}_3\".format(name),reuse=tf.AUTO_REUSE)\n",
    "        score = tf.nn.softmax(fc3 , axis=1)\n",
    "        ## define my softmax\n",
    "#         exp_fc3 = tf.exp(fc3)*mask\n",
    "#         exp_sum = tf.reduce_sum(exp_fc3,axis=1,keepdims=True)\n",
    "#         score = exp_fc3/exp_sum\n",
    "    \n",
    "    return score , tf.reduce_sum(inputs*score , axis=1)\n",
    "\n",
    "def attn_Encoder(inputs , mask , dim , name , init_state=None , t_len=20 , reuse=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(dim,name=name,reuse=reuse)\n",
    "    if init_state:\n",
    "        state = init_state\n",
    "    else:\n",
    "        state = [tf.zeros([tf.shape(inputs)[0] , cell.state_size[0]]),\n",
    "                 tf.zeros([tf.shape(inputs)[0] , cell.state_size[1]])]\n",
    "    output_seq = []\n",
    "    score_seq = []\n",
    "    for t in range(t_len):\n",
    "        score , attn_vec = attend_vector(inputs,state[1],mask,name=\"Encode\")\n",
    "        out , state = cell(attn_vec,state)\n",
    "        output_seq.append(out)\n",
    "        score_seq.append(score)\n",
    "    \n",
    "    return output_seq , state , score_seq \n",
    "\n",
    "\n",
    "def attn_Decoder(inputs , inputs_E , mask , dim , name , init_state=None , t_len=20 , reuse=False , stack_flag=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(dim,name=name,reuse=reuse)\n",
    "    if init_state:\n",
    "        state = init_state\n",
    "    else:\n",
    "        state = [tf.zeros([tf.shape(inputs)[0] , cell.state_size[0]]),\n",
    "                 tf.zeros([tf.shape(inputs)[0] , cell.state_size[1]])]\n",
    "    output_seq = []\n",
    "    score_seq = []\n",
    "    for t in range(t_len):\n",
    "        score , attn_vec = attend_vector(inputs_E,state[1],mask,name=\"Decode\")\n",
    "        if stack_flag:\n",
    "            attn_vec = tf.concat([attn_vec,inputs[:,t]] , axis=-1)\n",
    "        else:\n",
    "            attn_vec = tf.concat([attn_vec,inputs[t]] , axis=-1)\n",
    "        out , state = cell(attn_vec,state)\n",
    "        output_seq.append(out)\n",
    "        score_seq.append(score)\n",
    "    \n",
    "    return output_seq , state , score_seq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_clf(inputs,dim,embd):\n",
    "    fc1 = ly.fully_connected(inputs,dim,activation_fn=tf.nn.leaky_relu,scope=\"clf_fc1\",reuse=tf.AUTO_REUSE)\n",
    "    fc2 = ly.fully_connected(fc1,int(embd.shape[0]),activation_fn=None,scope=\"clf_fc2\",reuse=tf.AUTO_REUSE)\n",
    "    return fc2@embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_catece(x):\n",
    "    logit = x[0]\n",
    "    idx = x[1]\n",
    "    ce = []\n",
    "    for t in range(Decoder_max_len-1):\n",
    "        ce.append( tf.log(tf.nn.embedding_lookup(logit[t],idx[t])+1e-10) )\n",
    "    return tf.stack(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Building!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Seq_g = tf.Graph()\n",
    "embd_dim = 200\n",
    "L0_dim = 512\n",
    "L1_dim = 512\n",
    "L2_dim = 512\n",
    "clf_dim = 300\n",
    "\n",
    "with Seq_g.as_default():\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        _in = tf.placeholder(tf.int32,[None,None])\n",
    "        _in_mask = tf.placeholder(tf.float32,[None,None])\n",
    "        in_mask = tf.expand_dims(_in_mask,axis=-1)\n",
    "        \n",
    "        _in_length = tf.placeholder(tf.int32,[None])\n",
    "        \n",
    "        _out = tf.placeholder(tf.int32,[None,Decoder_max_len])\n",
    "        _out_mask = tf.placeholder(tf.float32,[None,Decoder_max_len])\n",
    "        gt = _out[:,1::]\n",
    "        gt_mask = _out_mask[:,1::]\n",
    "        \n",
    "        schedual_rate = tf.random_uniform([Decoder_max_len],maxval=1.0)\n",
    "        schedual_th = tf.placeholder(tf.float32)\n",
    "        infer_start = tf.ones([tf.shape(_in)[0]],dtype=tf.int32)\n",
    "        \n",
    "    with tf.name_scope(\"Embedding\"):\n",
    "        ## word embedding\n",
    "        _embd = tf.Variable(tf.truncated_normal([len(idx2word) , embd_dim],stddev=0.1),name=\"Word_Embd\")\n",
    "        _embd_T = tf.transpose(_embd,[1,0])\n",
    "        x_vector = tf.nn.embedding_lookup(_embd,_in,max_norm=5)\n",
    "        y_vector = tf.nn.embedding_lookup(_embd,_out,max_norm=5)\n",
    "        \n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"Encoder\"):\n",
    "        e_cell0 = tf.contrib.rnn.LSTMCell(L0_dim,name=\"E_layer_0\",reuse=False)\n",
    "        e_cell1 = tf.contrib.rnn.LSTMCell(L1_dim,name=\"E_layer_1\",reuse=False)\n",
    "        \n",
    "        E_layer_0 , E_state_0= tf.nn.dynamic_rnn(e_cell0,x_vector,sequence_length=_in_length,dtype=tf.float32)\n",
    "        E_layer_1 , E_state_1= tf.nn.dynamic_rnn(e_cell1,E_layer_0,sequence_length=_in_length,dtype=tf.float32)\n",
    "        \n",
    "    with tf.name_scope(\"Decoder\"):\n",
    "        \n",
    "        D_layer_0 , D_state_0 = Encoder(y_vector,L0_dim,\"rnn/E_layer_0\",init_state=E_state_0,reuse=True,\n",
    "                                        t_len=Decoder_max_len-1,stack_flag=True)\n",
    "#         D_layer_1 , D_state_1 = Encoder(D_layer_0,L1_dim,\"rnn/E_layer_1\",init_state=E_state_1,reuse=True,\n",
    "#                                         t_len=Decoder_max_len-1,stack_flag=False)\n",
    "        \n",
    "        D_layer_1 , D_state_1 , D_score = attn_Decoder(D_layer_0,E_layer_1,in_mask,L2_dim,name=\"Attn_D_layer_1\",\n",
    "                                                       init_state=E_state_1,t_len=Decoder_max_len-1,stack_flag=False)\n",
    "        \n",
    "        output_seq = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            choice_input = D_layer_1[t]\n",
    "            out = word_clf(choice_input,clf_dim,_embd_T)\n",
    "            output_seq.append(out)\n",
    "        _logits = tf.stack(output_seq,axis=1)\n",
    "        _prob = tf.nn.softmax(_logits,axis=-1)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"Loss\"):\n",
    "#         cross_entropy_0 = tf.map_fn(mask_catece,elems=(_prob,gt),dtype=tf.float32)\n",
    "#         cross_entropy = tf.reduce_sum(cross_entropy_0*gt_mask,axis=-1)/tf.reduce_sum(gt_mask,axis=-1)\n",
    "#         _loss = -tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        gt = tf.one_hot(gt,depth=len(idx2word),dtype=tf.float32)\n",
    "        cross_entropy_0 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(gt,[-1,len(idx2word)]),\n",
    "                                                                     logits=tf.reshape(_logits,[-1,len(idx2word)]))\n",
    "        cross_entropy_1 = tf.reshape(cross_entropy_0,[-1,Decoder_max_len-1])\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy_1*gt_mask,axis=-1)/tf.reduce_sum(gt_mask,axis=-1)\n",
    "        _loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "    with tf.name_scope(\"Train_strategy\"):\n",
    "        opt = tf.train.AdamOptimizer(1e-4)\n",
    "        _update = opt.minimize(_loss)\n",
    "    \n",
    "    with tf.name_scope(\"Inference\"):\n",
    "        ## start at Encoder layer 2 : E_layer2\n",
    "        infer_out = tf.nn.embedding_lookup(_embd,infer_start)\n",
    "        infer_state_0 = E_state_0\n",
    "        infer_state_1 = E_state_1\n",
    "        infer_state_2 = E_state_1\n",
    "        \n",
    "        infer_score_seq = []\n",
    "        infer_pred_idx_seq = []\n",
    "        infer_logits_seq = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            tmp = Encoder([infer_out],L0_dim,\"rnn/E_layer_0\",init_state=infer_state_0,reuse=True,\n",
    "                          t_len=1,stack_flag=False)\n",
    "            infer_layer_0 , infer_state_0 = tmp\n",
    "            \n",
    "            \n",
    "#             tmp = Encoder(infer_layer_0,L1_dim,\"rnn/E_layer_1\",init_state=infer_state_1,reuse=True,\n",
    "#                           t_len=1,stack_flag=False)\n",
    "#             infer_layer_1 , infer_state_1 = tmp\n",
    "            \n",
    "            tmp = attn_Decoder(infer_layer_0,E_layer_1,in_mask,L2_dim,name=\"Attn_D_layer_1\",\n",
    "                               init_state=infer_state_2,t_len=1,reuse=True,stack_flag=False)\n",
    "            \n",
    "            infer_layer_2 , infer_state_2 , infer_score = tmp\n",
    "            \n",
    "            infer_score_seq.append(infer_score)\n",
    "            \n",
    "            infer_out = word_clf(infer_layer_2[0],clf_dim,_embd_T)\n",
    "            infer_logits_seq.append(infer_out)\n",
    "            \n",
    "            out_index = tf.argmax(infer_out,axis=1)\n",
    "            infer_pred_idx_seq.append(out_index)\n",
    "            infer_out = tf.nn.embedding_lookup(_embd , out_index)\n",
    "            \n",
    "        infer_pred_idx_seq = tf.stack(infer_pred_idx_seq,axis=1)\n",
    "        infer_logits = tf.stack(infer_logits_seq,axis=1)\n",
    "        infer_prob = tf.nn.softmax(infer_logits,axis=-1)\n",
    "    \n",
    "    tf.summary.FileWriter(log_path,graph=Seq_g)\n",
    "    _init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=10,var_list=tf.global_variables())\n",
    "    \n",
    "print(\"Finish Building!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Start Training ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"### Start Training ###\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=Seq_g,config=sess_opt)\n",
    "sess.run(_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Attn_ver1/model_para/model_60.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,\"Attn_ver1/model_para/model_60.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(i):\n",
    "    tmp_end = max(length_in[i])\n",
    "    my_dict = {\n",
    "        _in:idx_in_sen[i,:tmp_end],\n",
    "        _in_mask:mask_in[i,:tmp_end],\n",
    "        _out:idx_out_sen[i],\n",
    "        _out_mask:mask_out[i],\n",
    "        _in_length:length_in[i]\n",
    "    }\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(sess,_pred,count=3):\n",
    "    idx = np.random.choice(idx_in_sen.shape[0],[count])\n",
    "    tmp_max_len = max(length_in[idx])\n",
    "    my_dict = {\n",
    "        _in:idx_in_sen[idx,:tmp_max_len],\n",
    "        _in_mask:mask_in[idx,:tmp_max_len],\n",
    "        _in_length:length_in[idx]\n",
    "    }\n",
    "    pred = sess.run(_pred , feed_dict=my_dict)\n",
    "    \n",
    "    word_seq = []\n",
    "    for i in range(3):\n",
    "        idx_sen = pred[i]\n",
    "        tmp = []\n",
    "        for t in range(Decoder_max_len-1):\n",
    "            if(idx_sen[t] == 3):\n",
    "                break\n",
    "            tmp.append(idx2word[idx_sen[t]])\n",
    "        word_seq.append(tmp)\n",
    "    \n",
    "    print(\"Max length :\" , tmp_max_len)\n",
    "    for i in range(3):\n",
    "        print(\"  Input word  :\" , data_agent.orig_data[remain_idx[idx[i]]])\n",
    "        print(\"  Input index :\" , idx_in_sen[idx[i],:tmp_max_len])\n",
    "        print(\"  Ground word :\" , data_agent.out_sen[remain_idx[idx[i]]])\n",
    "        print(\"    Output    :\" , word_seq[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epoch = 60\n",
    "n_step = idx_in_sen.shape[0]//batch_size\n",
    "\n",
    "r_index = np.arange(idx_in_sen.shape[0])\n",
    "loss_list = []\n",
    "try:\n",
    "    for e in range(1,n_epoch+1):\n",
    "        np.random.shuffle(r_index)\n",
    "        start_time = time.time()\n",
    "        start = 0\n",
    "        for s in range(n_step):\n",
    "            idx = r_index[start:start+batch_size]\n",
    "            _,l = sess.run([_update,_loss] , feed_dict=get_batch(idx))\n",
    "            start += batch_size\n",
    "            print(\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\".format(s,l,time.time()-start_time) , end=\"\\r\")\n",
    "            if s % 500 == 0:\n",
    "                print(\"step {:>5d} loss : {:>9.4f} time : {:>7.2f}\".format(s,l,time.time()-start_time) , end=\"\\n\")\n",
    "                evaluate_batch(sess,infer_pred_idx_seq,3)\n",
    "\n",
    "        loss_list.append(l)\n",
    "        print(\"\\nEpoch {0:>3d}/{1:d} loss : {2:>9.4f} time : {3:>8.2f}\".format(e,n_epoch,l,time.time()-start_time))\n",
    "\n",
    "        evaluate_batch(sess,infer_pred_idx_seq,3)\n",
    "\n",
    "        if e%4 == 0:\n",
    "            saver.save(sess,os.path.join(model_path,\"model_{}.ckpt\".format(e)))\n",
    "except KeyboardInterrupt :\n",
    "    saver.save(sess,os.path.join(model_path,\"model_{}.ckpt\".format(\"lastest\")))\n",
    "    pickle.dump(loss_list,open(os.path.join(log_path,\"loss.pkl\") , \"wb\"))\n",
    "    print()\n",
    "    print(\"Save loss history...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Training Finished!! ###\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_test_data(sess,_pred,x,word2idx,batch_size=1000):\n",
    "    start = 0\n",
    "    pred_word_seq = []\n",
    "    while(start < len(x)):\n",
    "        batch_idx_sen = []\n",
    "        batch_length = []\n",
    "        batch_mask = []\n",
    "        max_len = 0\n",
    "        for s in x[start : start+batch_size]:\n",
    "            l = len(s)-1\n",
    "            if(l>max_len):\n",
    "                max_len = l\n",
    "        \n",
    "        for s in x[start : start+batch_size]:\n",
    "            arr = []\n",
    "            ## s[1::] : remove first word \"SOS\"\n",
    "            batch_mask.append( np.zeros([max_len]))\n",
    "            batch_mask[-1][0:len(s[1::])] += 1 \n",
    "            batch_length.append(len(s[1::]))\n",
    "            for ss in s[1::]:\n",
    "                try:\n",
    "                    arr.append(word2idx[ss])\n",
    "                except:\n",
    "                    arr.append(1)\n",
    "            arr.extend([0]*(max_len-len(s[1::])))\n",
    "            batch_idx_sen.append(arr)\n",
    "        batch_idx_sen = np.array(batch_idx_sen)\n",
    "        batch_length = np.array(batch_length)\n",
    "        batch_mask = np.stack(batch_mask)\n",
    "        \n",
    "        pred_sen = sess.run(_pred,feed_dict={\n",
    "            _in:batch_idx_sen,\n",
    "            _in_length:batch_length,\n",
    "            _in_mask:batch_mask\n",
    "        })\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            idx_sen = pred_sen[i]\n",
    "            tmp = []\n",
    "            for t in range(Decoder_max_len-1):\n",
    "                if(idx_sen[t] == 3):\n",
    "                    break\n",
    "                elif(idx_sen[t] == 1):\n",
    "                    tmp.append(np.random.choice(idx2word))\n",
    "                else:\n",
    "                    tmp.append(idx2word[idx_sen[t]])\n",
    "            pred_word_seq.append(\" \".join(tmp))\n",
    "        start += batch_size\n",
    "        \n",
    "    return pred_word_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_infer = infer_test_data(sess,infer_pred_idx_seq,test_agent.orig_data,word2idx,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Infer samples :\")\n",
    "for i in np.random.choice(len(test_agent.orig_data) , 10 , replace=False):\n",
    "    print(\"  Input : \" , \" \".join(test_agent.orig_data[i]))\n",
    "    print(\"  Infer : \" , test_infer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_infer(data,name):\n",
    "    path = os.path.join(exp_folder,name)\n",
    "    print(\"Save at '{}'\".format( path))\n",
    "    with open( path, \"w\") as f:\n",
    "        for s in data:\n",
    "            s = \"\".join(s.split())\n",
    "            if(len(s) == 0):\n",
    "                s = np.random.choice(idx2word[4::])\n",
    "            f.write(s+\"\\n\")\n",
    "\n",
    "def save_infer_seg(data,name):\n",
    "    path = os.path.join(exp_folder,name)\n",
    "    print(\"Save at '{}'\".format( path))\n",
    "    with open( path, \"w\") as f:\n",
    "        for s in data:\n",
    "            if(len(s) == 0):\n",
    "                s = np.random.choice(idx2word[4::])\n",
    "            f.write(s+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_infer(test_infer,\"infer_output.txt\")\n",
    "save_infer_seg(test_infer,\"infer_seg.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "dsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
